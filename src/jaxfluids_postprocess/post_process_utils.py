import glob
import os
import subprocess
from typing import Dict, List, Tuple, Union
from warnings import warn

import h5py
import numpy as np
import time
import jax.numpy as jnp
from jax import Array
import jax

from jaxfluids.domain.helper_functions import (
    reassemble_cell_centers, reassemble_cell_faces,
    reassemble_cell_sizes, reassemble_buffer)
from jaxfluids.config import precision

def load_data(
        path: str,
        quantities: List[str],
        start: int = 0,
        stop: int = None,
        step: int = 1,
        verbose: bool = True,
        reassemble = True,
        reassemble_jnp = False,
        ) -> Tuple[Tuple[np.ndarray, np.ndarray, np.ndarray],
                   Tuple[np.ndarray, np.ndarray, np.ndarray],
                   np.ndarray, Dict[str, np.ndarray]]:
    # TODO should we also return metadata?
    """Loads .h5 result files generated by a jaxfluids simulation.

    :param path: Path to the folder containing the .h5 files for the time snapshots
    :type path: str
    :param quantities: Quantities to load
    :type quantities: List
    :param start: Starting time snapshot to load, defaults to 0
    :type start: int, optional
    :param stop: Ending time snapshot to load, defaults to None
    :type stop: int, optional
    :param step: Interval of time snapshots to load, defaults to 1
    :type step: int, optional
    :return: Cell centers coordinates, cell sizes, times and dictionary of buffers
    :rtype: Tuple
    """
    assert os.path.exists(path), f"Given path {path:s} does not exist."
    if isinstance(stop, int):
        assert stop > start, "Stop argument in load_data must be larger than start."
    is_h5files = len(glob.glob(os.path.join(path, "*.h5"))) > 0
    if path.endswith("domain"):
        assert is_h5files, f"No h5 files found in {path:s}."
    else:
        if not is_h5files:
            path_ = os.path.join(path, "domain")
            is_h5files = len(glob.glob(os.path.join(path_, "*.h5"))) > 0
            assert is_h5files, f"No h5 files in {path:s} or in {path_:s}"
            path = path_

    # SEARCH DIRECTORY FOR FILES
    files = []
    times = []
    for file in os.listdir(path):
        if file.endswith("h5"):
            if "nan" in file:
                continue 
            files.append(file)
            times.append(float(os.path.splitext(os.path.basename(file))[0][5:]))
    indices     = np.argsort(np.array(times))
    times       = np.array(times)[indices][start:stop:step]
    files       = np.array(files)[indices][start:stop:step]
    no_times    = len(times)

    # METADATA AND GRID
    with h5py.File(os.path.join(path, files[0]), "r") as h5file:
        # READ DOMAIN
        x = h5file["domain/gridX"][()]
        y = h5file["domain/gridY"][()]
        z = h5file["domain/gridZ"][()]
        dx = h5file["domain/cellsizeX"][()]
        dy = h5file["domain/cellsizeY"][()]
        dz = h5file["domain/cellsizeZ"][()]
        fx = h5file["domain/gridFX"][()]
        fy = h5file["domain/gridFY"][()]
        fz = h5file["domain/gridFZ"][()]
        split_factors = tuple(h5file["domain"]["split_factors"][:])

        # READ METADATA
        is_parallel = h5file["metadata"]["is_parallel"][()]
        diffuse_interface_model = h5file["metadata"]["diffuse_interface_model"][()]
        if type(diffuse_interface_model) != np.bool_:
            diffuse_interface_model = diffuse_interface_model.decode("utf-8")
        levelset_model = h5file["metadata"]["levelset_model"][()]
        if type(levelset_model) != np.bool_:
            levelset_model = levelset_model.decode("utf-8")
        if "fluid_names" in h5file["metadata"].keys():
            fluid_names = tuple([fluid_name.decode("utf-8") for fluid_name in h5file["metadata"]["fluid_names"][()]])
        else:
            warn("""The keyword 'fluid_names' in output h5 files will be mandatory
                in upcoming JAX-Fluids versions.\n\n""", DeprecationWarning,
                stacklevel=2)
            fluid_names = None
        number_fluids = h5file["metadata"]["number_fluids"][()]
        is_double = h5file["metadata"]["is_double_precision"][()]

        available_quantities = {}
        for key in h5file["metadata/available_quantities"].keys():
            quantity_array = h5file["metadata/available_quantities"][key][:]
            available_quantities[key] = [arr.decode("utf-8") for arr in quantity_array]
        
        metadata_dict = {
            "is_parallel": is_parallel,
            "diffuse_interface_model": diffuse_interface_model,
            "levelset_model": levelset_model,
            "fluid_names": fluid_names,
            "number_fluids": number_fluids,
            "is_double_precision": is_double,
            "available_quantities": available_quantities,
        }

        if is_double:
            precision.enable_double_precision()
        dtype = np.float64 if is_double else np.float32

        cell_centers = (x,y,z)
        cell_faces = (fx,fy,fz)
        cell_sizes = (dx,dy,dz)

        cell_centers: Tuple[np.ndarray] = [np.squeeze(xi) for xi in cell_centers]
        cell_faces: Tuple[np.ndarray] = [np.squeeze(fxi) for fxi in cell_faces]
        cell_sizes: Tuple[np.ndarray] = [np.squeeze(dxi) for dxi in cell_sizes]

        if is_parallel:
            cell_centers = reassemble_cell_centers(cell_centers, split_factors)
            cell_sizes = reassemble_cell_sizes(cell_sizes, split_factors)
            cell_faces = reassemble_cell_faces(cell_faces, split_factors)

        dim = sum([1 if xi.size > 1 else 0 for xi in cell_centers])
        nx = cell_centers[0].size
        ny = cell_centers[1].size
        nz = cell_centers[2].size

        number_of_cells = (nx,ny,nz)
        number_of_cells_device = tuple([int(number_of_cells[i]/split_factors[i]) for i in range(3)])

    # CHECK IF REQUESTED QUANTITIES ARE AVAILABLE
    quantity_to_field_type = {}
    for quantity in quantities:
        check = False
        # IDENTIFY FIELD TYPE
        if quantity.split("_")[0] == "real":
            if quantity.split("_")[1] in available_quantities["real_fluid"]:
                quantity_to_field_type[quantity] = "real_fluid"
                check = True
        else:
            for field_type in available_quantities:
                if field_type == "real_fluid":
                    continue
                if quantity in available_quantities[field_type]:
                    quantity_to_field_type[quantity] = field_type
                    check = True

        assert check, f"Quantity {quantity} not available in .h5 file"

    if is_parallel and not reassemble:
        # raise NotImplementedError #TODO AARON PARALLEL PARAVIEW TRANSPOSE
        nx,ny,nz = number_of_cells_device
        no_subdomains = np.prod(np.array(split_factors))
        shape = (no_subdomains,nx,ny,nz)
    else:
        nx,ny,nz = number_of_cells
        shape = (nx,ny,nz)

    # CREATE BUFFERS
    data_dictionary = {}
    for quantity in quantities:
        field_type = quantity_to_field_type[quantity]

        # PRIMITIVES AND CONSERVATIVES
        if field_type in ["primitives", "conservatives"]:
            if levelset_model == "FLUID-FLUID":
                if quantity in ["velocity", "momentum"]:
                    data_dictionary[quantity] = np.zeros((no_times, 2, dim) + shape, dtype)
                else:
                    data_dictionary[quantity] = np.zeros((no_times, 2) + shape, dtype)

            else:
                if quantity in ["velocity", "momentum"]:
                    data_dictionary[quantity] = np.zeros((no_times, dim) + shape, dtype)
                else:
                    data_dictionary[quantity] = np.zeros((no_times,) + shape, dtype)

        # REAL FLUID
        elif field_type in ["levelset", "real_fluid"]:
            if quantity in ["normal", "real_velocity", "real_momentum"]:
                data_dictionary[quantity] = np.zeros((no_times, dim) + shape, dtype)
            elif quantity == "interface_pressure":
                data_dictionary[quantity] = np.zeros((no_times, 2) + shape, dtype)
            elif quantity == "interface_velocity" and levelset_model == "FLUID-SOLID-DYNAMIC-COUPLED":
                data_dictionary[quantity] = np.zeros((no_times, dim) + shape, dtype)
            else:
                data_dictionary[quantity] = np.zeros((no_times,) + shape, dtype)

        # MISCELLANEOUS
        elif field_type == "miscellaneous":
            if quantity == "vorticity":
                data_dictionary[quantity] = np.zeros((no_times, 3) + shape, dtype)
            else:
                data_dictionary[quantity] = np.zeros((no_times,) + shape, dtype)

        # MASS FLOW FORCING
        elif field_type == "forcings":
            data_dictionary[quantity] = np.zeros(no_times)


    # FILL BUFFERS
    for i, file in enumerate(files):
        if verbose:
            print(f"Loading time snapshot {times[i]:.4e}")
        with h5py.File(os.path.join(path, file), "r") as h5file:
            
            for quantity in quantities:

                field_type = quantity_to_field_type[quantity]
                
                # PRIMITIVES
                if field_type == "primitives":
                    for j in range(2 if levelset_model == "FLUID-FLUID" else 1):
                        quantity_name_h5 = quantity + "_%d" % j if levelset_model == "FLUID-FLUID" else quantity
                        s_1 = np.s_[i,j] if levelset_model == "FLUID-FLUID" else np.s_[i]
                        if is_parallel:
                            buffer = h5file["primitives/" + quantity_name_h5][:]
                            if reassemble:
                                buffer = reassemble_buffer(buffer, split_factors, is_transpose=True)
                            else:
                                buffer = jnp.swapaxes(buffer, -3, -1)
                        else:
                            buffer = h5file["primitives/" + quantity_name_h5][:].T
                        data_dictionary[quantity][s_1] = buffer

                # CONSERVATIVES
                elif field_type == "conservatives":
                    for j in range(2 if levelset_model == "FLUID-FLUID" else 1):
                        quantity_name_h5 = quantity + "_%d" % j if levelset_model == "FLUID-FLUID" else quantity
                        s_1 = np.s_[i,j] if levelset_model == "FLUID-FLUID" else np.s_[i]
                        if is_parallel:
                            buffer = h5file["conservatives/" + quantity_name_h5][:]
                            if reassemble:
                                buffer = reassemble_buffer(buffer, split_factors, is_transpose=True)
                            else:
                                buffer = jnp.swapaxes(buffer, -3, -1)
                        else:
                            buffer = h5file["conservatives/" + quantity_name_h5][:].T
                        data_dictionary[quantity][s_1] = buffer

                # LEVELSET
                elif field_type == "levelset":
                    if is_parallel:
                        buffer = h5file["levelset/" + quantity][:]
                        if reassemble:
                            buffer = reassemble_buffer(buffer, split_factors, is_transpose=True)
                        else:
                            buffer = jnp.swapaxes(buffer, -3, -1)
                    else:
                        buffer = h5file["levelset/" + quantity][:].T
                    data_dictionary[quantity][i] = buffer

                # REAL FLUID
                elif field_type == "real_fluid":
                    quantity_name_h5 = quantity.split("_")[1]
                    if is_parallel:
                        buffer = h5file["real_fluid/" + quantity_name_h5][:]
                        if reassemble:
                            buffer = reassemble_buffer(buffer, split_factors, is_transpose=True)
                        else:
                            buffer = jnp.swapaxes(buffer, -3, -1)
                    else:
                        buffer = h5file["real_fluid/" + quantity_name_h5][:].T
                    data_dictionary[quantity][i] = buffer

                # MISCELLANEOUS
                elif field_type == "miscellaneous":
                    if is_parallel:
                        buffer = h5file["miscellaneous/" + quantity][:]
                        if reassemble:
                            buffer = reassemble_buffer(buffer, split_factors, is_transpose=True)
                        else:
                            buffer = jnp.swapaxes(buffer, -3, -1)
                    else:
                        buffer = h5file["miscellaneous/" + quantity][:].T
                    data_dictionary[quantity][i] = buffer                

                # MASS FLOW FORCING
                elif field_type == "forcings":
                    data_dictionary[quantity][i] = h5file["forcings/mass_flow/force_scalar"][()]

    return cell_centers, cell_sizes, times, data_dictionary

def load_statistics(
        path: str,
        start: int = 0,
        stop: int = None,
        step: int = 1,
        verbose: bool = True,
        ) -> Tuple[List, List, Dict]:
    """Loads .h5 statistics files generated by a jaxfluids simulation.

    :param path: Path to the folder containing the .h5 files for the time snapshots
    :type path: str
    :param quantities: Quantities to load
    :type quantities: List
    :param start: Starting time snapshot to load, defaults to 0
    :type start: int, optional
    :param stop: Ending time snapshot to load, defaults to None
    :type stop: int, optional
    :param N: Interval of time snapshots to load, defaults to 1
    :type N: int, optional
    :return: Cell centers coordinates, cell sizes, times and dictionary of buffers
    :rtype: Tuple
    """
    assert os.path.exists(path), f"Given path {path:s} does not exist."
    is_h5files = len(glob.glob(os.path.join(path, "*.h5"))) > 0
    if path.endswith("statistics"):
        assert is_h5files, f"No h5 files found in {path:s}."
    else:
        if not is_h5files:
            path_ = os.path.join(path, "statistics")
            is_h5files = len(glob.glob(os.path.join(path_, "*.h5"))) > 0
            assert is_h5files, f"No h5 files in {path:s} or in {path_:s}."
            path = path_

    # SEARCH DIRECTORY FOR FILES
    files = []
    times = []
    for file in os.listdir(path):
        if file.endswith("h5"):
            if "nan" in file:
                continue
            files.append(file)
            times.append(float(os.path.splitext(os.path.basename(file))[0][11:]))
    indices = np.argsort(np.array(times))
    times = np.array(times)[indices][start:stop:step]
    files = np.array(files)[indices][start:stop:step]
    no_times = len(times)

    # METADATA AND GRID
    with h5py.File(os.path.join(path, files[0]), "r") as h5file:
        print(h5file.keys())

        x = h5file["domain/gridX"][:]
        y = h5file["domain/gridY"][:]
        z = h5file["domain/gridZ"][:]
        # dx = h5file["domain/cellsizeX"][()]
        # dy = h5file["domain/cellsizeY"][()]
        # dz = h5file["domain/cellsizeZ"][()]
        split_factors = tuple(h5file["domain"]["split_factors"][:])

        turbulent_case = h5file["metadata"]["turbulent_case"][()].decode("utf-8")
        assert turbulent_case in ["HIT", "CHANNEL"], (f"Turbulent case {turbulent_case} is"
            " currently not supported.")
        is_parallel = h5file["metadata"]["is_parallel"][()]
        is_double = h5file["metadata"]["is_double_precision"]
        dtype = np.float64 if is_double else np.float32

        cell_centers = [x,y,z]
        if is_parallel:
            cell_centers = reassemble_cell_centers(cell_centers, split_factors)
        # cell_sizes = [dx,dy,dz]
        dim = sum([1 if len(xi) > 1 else 0 for xi in cell_centers])
        nx = cell_centers[0].size
        ny = cell_centers[1].size
        nz = cell_centers[2].size

        sample_quantities = list(h5file["samples"].keys())
        mean_quantities = list(h5file["means"].keys())
        fluctuating_quantities = list(h5file["fluctuations"].keys())

    # CREATE BUFFERS
    statistics_dictionary = {"means": {}, "fluctuations": {}, "samples": {}}
    for quantity in sample_quantities:
        statistics_dictionary["samples"][quantity] = np.zeros((no_times), dtype)

    for quantity in mean_quantities:
        if turbulent_case == "HIT":
            statistics_dictionary["means"][quantity] = np.zeros((no_times), dtype)
        if turbulent_case == "CHANNEL":
            statistics_dictionary["means"][quantity] = np.zeros((no_times, ny), dtype)

    for quantity in fluctuating_quantities:
        if turbulent_case == "HIT":
            statistics_dictionary["fluctuations"][quantity] = np.zeros((no_times), dtype)
        if turbulent_case == "CHANNEL":
            statistics_dictionary["fluctuations"][quantity] = np.zeros((no_times, ny), dtype)

    # FILL BUFFERS
    for i, file in enumerate(files):
        if verbose:
            print("Loading time snapshot %.4e" % times[i])
        with h5py.File(os.path.join(path, file), "r") as h5file:

            for quantity in sample_quantities:
                buffer = h5file["samples/" + quantity][()]
                statistics_dictionary["samples"][quantity][i] = buffer

            for quantity in mean_quantities:
                if turbulent_case in ["HIT"]:
                    buffer = h5file["means/" + quantity][()]
                if turbulent_case in ["CHANNEL"]:
                    buffer = h5file["means/" + quantity][:]
                statistics_dictionary["means"][quantity][i] = buffer

            for quantity in fluctuating_quantities:
                if turbulent_case in ["HIT"]:
                    buffer = h5file["fluctuations/" + quantity][()]
                if turbulent_case in ["CHANNEL"]:
                    buffer = h5file["fluctuations/" + quantity][:]
                statistics_dictionary["fluctuations"][quantity][i] = buffer

    return cell_centers, times, statistics_dictionary

def reassemble_parallel_data(
        path_read: str,
        path_save: str,
        quantity_list: List[str] = None,
        start: int = 0,
        stop: int = None,
        step: int = 1,
        jax_numpy = False
        ) -> None:
    """Reads parallel (single or multihost)
    hdf5 output, reassembles the buffers and saves as hdf5.

    :param path: _description_
    :type path: str
    :param quantity_list: _description_
    :type quantity_list: str
    :param start: _description_, defaults to 0
    :type start: int, optional
    :param stop: _description_, defaults to None
    :type stop: int, optional
    """

    # TIMESNAPSHOTS AND METADATA
    files = []
    times = []

    for file in os.listdir(path_read):
        if file.endswith("h5"):
            if "nan" in file:
                continue 
            files.append(file)
            time_string = file.split("_")[-1]
            time_string = os.path.splitext(time_string)[0]
            times.append(time_string)

    if "proc" in files[0]:
        is_multihost = True
    else:
        is_multihost = False

    indices = np.argsort(np.array(times))
    times = np.array(times)[indices]
    files = np.array(files)[indices]

    if is_multihost:
        times = np.unique(times)
        no_hosts = 0
        for file in files:
            if times[0] in file:
                no_hosts += 1
    else:
        no_hosts = 1

    times = times[start:stop:step]
    no_times = len(times)
    print("%d Time snapshots: " % no_times, times)

    # REASSEMBLE CELL CENTERS AND CELL SIZES
    cell_centers_list = [[], [], []]
    cell_faces_list = [[], [], []]
    cell_sizes_list = [[], [], []]
    for host in range(no_hosts):
        
        if is_multihost:
            path_host = os.path.join(path_read, "data_proc%s_%s.h5" % (str(host), times[0]))
        else:
            path_host = os.path.join(path_read, "data_%s.h5" % (times[0]))

        with h5py.File(path_host, "r") as h5file:

            is_parallel = h5file["metadata/is_parallel"][()]
            assert is_parallel, "Reassemble parallel data requires split data."
            is_double = h5file["metadata/is_double_precision"][()]
            if is_double:
                precision.enable_double_precision()

            if host == 0:
                available_quantities = {}
                for key in h5file["metadata/available_quantities"].keys():
                    quantity_array = h5file["metadata/available_quantities"][key][:]
                    available_quantities[key] = [arr.decode("utf-8") for arr in quantity_array]
                split_factors = h5file["domain/split_factors"][:]
                levelset_model = h5file["metadata"]["levelset_model"][()]
                if type(levelset_model) != np.bool_:
                    levelset_model = levelset_model.decode("utf-8")

            for axis_index, grid in enumerate(["gridFX", "gridFY", "gridFZ"]):
                cell_faces_list[axis_index].append(h5file["domain/%s" % grid][()])
                
            for axis_index, grid in enumerate(["gridX", "gridY", "gridZ"]):
                cell_centers_list[axis_index].append(h5file["domain/%s" % grid][()])

            for axis_index, grid in enumerate(["cellsizeX", "cellsizeY", "cellsizeZ"]):
                cell_sizes_list[axis_index].append(h5file["domain/%s" % grid][()])

    cell_centers = [np.concatenate(cell_centers_list[i], axis=0) for i in range(3)]
    cell_faces = [np.concatenate(cell_faces_list[i], axis=0) for i in range(3)]
    cell_sizes = [np.concatenate(cell_sizes_list[i], axis=0) for i in range(3)]

    cell_centers = [np.squeeze(xi) for xi in cell_centers]
    cell_faces = [np.squeeze(fxi) for fxi in cell_faces]
    cell_sizes = [np.squeeze(dxi) for dxi in cell_sizes]

    cell_centers = reassemble_cell_centers(cell_centers, split_factors)
    cell_faces = reassemble_cell_faces(cell_faces, split_factors)
    cell_sizes = reassemble_cell_sizes(cell_sizes, split_factors)

    if not os.path.exists(path_save):
        os.mkdir(path_save)

    # IDENTIFY QUANTITIES TO LOAD
    quantities_to_load: Dict[str, List[str]] = {}
    if quantity_list != None:
        for quantity in quantity_list:
            check = False
            if quantity.split("_")[0] == "real":
                quantity_name = quantity.split("_")[-1]
                if quantity_name in available_quantities["real_fluid"]:
                    check = True
                    if "real_fluid" not in quantities_to_load:
                        quantities_to_load["real_fluid"] = []
                    quantities_to_load["real_fluid"].append(quantity_name)

            else:
                for field_type in available_quantities:
                    if quantity in available_quantities[field_type]:
                        check = True
                        if field_type not in quantities_to_load:
                            quantities_to_load[field_type] = []
                        if field_type in ("primitives", "conservatives") and levelset_model == "FLUID-FLUID":
                            for i in range(2):
                                quantity_name = quantity + "_%d" % i
                                quantities_to_load[field_type].append(quantity_name)
                        else:
                            quantities_to_load[field_type].append(quantity)

            assert check, f"Quantity {quantity} not available in .h5 file"

    else:
        for field_type in available_quantities:
            quantities_to_load[field_type] = []
            for quantity in available_quantities[field_type]:
                if field_type in ("primitives", "conservatives") and levelset_model == "FLUID-FLUID":
                    for i in range(2):
                        quantity_name = quantity + f"_{i:d}"
                        quantities_to_load[field_type].append(quantity_name)
                else:
                    quantities_to_load[field_type].append(quantity)


    for i, time_string in enumerate(times):

        filename_save = f"data_{time_string:s}.h5" 
        h5file_save = h5py.File(os.path.join(path_save, filename_save), "w")

        # REASSEMBLE AND SAVE FIELD BUFFERS
        start = time.time()
        for field in quantities_to_load:
            h5file_save.create_group(field)
            for quantity in quantities_to_load[field]:
                buffer_list = []
                for host in range(no_hosts):

                    if is_multihost:
                        path_host = os.path.join(path_read, "data_proc%s_%s.h5" % (str(host), time_string))
                    else:
                        path_host = os.path.join(path_read, "data_%s.h5" % (time_string))

                    with h5py.File(path_host, "r") as h5file:
                        buffer = h5file[field][quantity][:]
                        buffer_list.append(buffer)

                buffer = jnp.concatenate(buffer_list, axis=0)
                buffer = reassemble_buffer(buffer, tuple(split_factors), is_transpose=True, keep_transpose=True)
                h5file_save[field].create_dataset(data=buffer, name=quantity)

        # SAVE REMAINING DATA
        if is_multihost:
            h5file_read = h5py.File(os.path.join(path_read, "data_proc0_%s.h5" % time_string), "r")
        else:
            h5file_read = h5py.File(os.path.join(path_read, "data_%s.h5" % time_string), "r")

        h5file_read.copy("metadata", h5file_save)
        h5file_save["metadata/is_parallel"][()] = False
        h5file_save["metadata/is_multihost"][()] = False
        h5file_save["metadata/global_device_count"][()] = 1
        h5file_save["metadata/local_device_count"][()] = 1
        h5file_save.create_dataset(name="domain/cellsizeX", data=cell_sizes[0])
        h5file_save.create_dataset(name="domain/cellsizeY", data=cell_sizes[1])
        h5file_save.create_dataset(name="domain/cellsizeZ", data=cell_sizes[2])
        h5file_save.create_dataset(name="domain/gridX", data=cell_centers[0])
        h5file_save.create_dataset(name="domain/gridY", data=cell_centers[1])
        h5file_save.create_dataset(name="domain/gridZ", data=cell_centers[2])
        h5file_save.create_dataset(name="domain/gridFX", data=cell_faces[0])
        h5file_save.create_dataset(name="domain/gridFY", data=cell_faces[1])
        h5file_save.create_dataset(name="domain/gridFZ", data=cell_faces[2])
        h5file_save.create_dataset(name="domain/split_factors", data=np.array([1,1,1]))
        h5file_save.create_dataset(name="domain/dim", data=h5file_read["domain/dim"])
        h5file_save.create_dataset(name="time", data=h5file_read["time"])
        h5file_save.close()
        wall_clock = time.time() - start

        print("%4d/%4d -- Reassemble took %.5fs" % (i+1, len(times), wall_clock))

    create_xdmf_from_h5(path_save)


def create_xdmf_from_h5(path: str) -> None:
    """Creates .xdmf files from .h5 files for Paraview visualization.

    :param path: Path to .h5 files.
    :type path: str
    """

    # SEARCH DIRECTORY FOR FILES
    files = []
    times = []
    for file in os.listdir(path):
        if file.endswith("h5"):
            if "nan" in file:
                continue 
            files.append(file)
    files = np.array(files)

    xdmf_timeseries = []

    for file in files:

        # DOMAIN INFORMATION
        with h5py.File(os.path.join(path, file), "r") as h5file:
            x = h5file["domain/gridX"][()]
            y = h5file["domain/gridY"][()]
            z = h5file["domain/gridZ"][()]
            fx = h5file["domain/gridFX"][()]
            fy = h5file["domain/gridFY"][()]
            fz = h5file["domain/gridFZ"][()]
            number_of_cells = [x.size, y.size, z.size]
            is_double = h5file["metadata/is_double_precision"][()]

        h5file_name = file
        xdmffile_path = file[:-3] + ".xdmf"
        print("Writing file %s" % xdmffile_path)

        xdmffile_path = os.path.join(path, xdmffile_path)

        # XDMF QUANTITIES
        xdmf_quants = []

        with h5py.File(os.path.join(path, file), "r") as h5file:
            
            physical_simulation_time = h5file["time"][()]

            # CONSERVATIVES AND PRIMITIVES 
            for key in ["conservatives", "primitives", "real_fluid", "miscellaneous", "levelset"]: 
                if key in h5file.keys():
                    for quantity in h5file[key]:
                        xdmf_quants.append(get_xdmf(key, quantity, h5file_name, *number_of_cells, is_double))

        xdmf_str = ""

        # XDMF START
        xdmf_preamble ='''<?xml version="1.0" ?>
        <!DOCTYPE Xdmf SYSTEM "Xdmf.dtd" []>
        <Xdmf Version="3.0">
        <Domain>
            <Grid Name="TimeStep" GridType="Collection" CollectionType="Temporal">'''
        
        xdmf_str_start = '''
                <Grid Name="SpatialData_%e" GridType="Uniform">
                    <Time TimeType="Single" Value="%e" />

                    <Geometry Type="VXVYVZ">
                        <DataItem Format="HDF" NumberType="Float" Precision="%i" Dimensions="%i">%s:domain/gridFX</DataItem>
                        <DataItem Format="HDF" NumberType="Float" Precision="%i" Dimensions="%i">%s:domain/gridFY</DataItem>
                        <DataItem Format="HDF" NumberType="Float" Precision="%i" Dimensions="%i">%s:domain/gridFZ</DataItem>
                    </Geometry>
                    <Topology Dimensions="%i %i %i" Type="3DRectMesh"/>''' %( # 1 512 128
                        physical_simulation_time, physical_simulation_time,
                        8 if is_double else 4, len(fx), h5file_name, 
                        8 if is_double else 4, len(fy), h5file_name, 
                        8 if is_double else 4, len(fz), h5file_name,
                        len(fx), len(fy), len(fz))
        
        xdmf_str_end = '''</Grid>'''

        # XDMF END
        xdmf_postamble = '''</Grid>
        </Domain>
        </Xdmf>'''

        # APPEND XDMF SPATIAL TO TIMESERIES
        xdmf_timeseries.append("\n".join([xdmf_str_start] + xdmf_quants + [xdmf_str_end]))

        # JOIN FINAL XDMF STR AND WRITE TO FILE
        xdmf_str = "\n".join([xdmf_preamble, xdmf_str_start] + xdmf_quants + [xdmf_str_end, xdmf_postamble])
        with open(xdmffile_path, "w") as xdmf_file:
            xdmf_file.write(xdmf_str)

    write_timeseries(path, xdmf_timeseries)
    
def get_xdmf(
        group: str,
        quantity: str,
        h5file_name: str,
        Nx: int,
        Ny: int,
        Nz: int,
        is_double: bool
        ) -> str:
    """_summary_
    TODO aaron
    :param group: _description_
    :type group: str
    :param quantity: _description_
    :type quantity: str
    :param h5file_name: _description_
    :type h5file_name: str
    :param Nx: _description_
    :type Nx: int
    :param Ny: _description_
    :type Ny: int
    :param Nz: _description_
    :type Nz: int
    :param is_double: _description_
    :type is_double: bool
    :return: _description_
    :rtype: str
    """
    dim_ = sum(1 if n > 1 else 0 for n in [Nx,Ny,Nz])
    if quantity in ["velocity", "momentum", "vorticity",
                    "velocity_0", "momentum_0",
                    "velocity_1", "momentum_1",
                    "interface_pressure"]:
        dim = dim_
        if quantity == "interface_pressure":
            dim = 2
        xdmf ='''<Attribute Name="%s" AttributeType="Vector" Center="Cell">
        <DataItem Format="HDF" NumberType="Float" Precision="%i" Dimensions="%i %i %i %i">%s:%s/%s</DataItem>
        </Attribute>''' %(quantity, 8 if is_double else 4, Nz, Ny, Nx, dim, h5file_name, group, quantity)
    else:
        xdmf ='''<Attribute Name="%s" AttributeType="Scalar" Center="Cell">
            <DataItem Format="HDF" NumberType="Float" Precision="%i" Dimensions="%i %i %i">%s:%s/%s</DataItem>
        </Attribute>''' %(quantity, 8 if is_double else 4, Nz, Ny, Nx, h5file_name, group, quantity)
    return xdmf

def write_timeseries(
        path: str,
        xdmf_timeseries: List
        ) -> None:
    """Write xdmffile for the complete time series so that visualization 
    tools like Paraview can load the complete time series at once. This is
    done once at the end of a simulation when every output time stamp is 
    known. 
    """
    xdmffile_path = os.path.join(path, "data_time_series.xdmf")

    xdmf_str = ""

    # XDMF START
    xdmf_str_start ='''<?xml version="1.0" ?>
    <!DOCTYPE Xdmf SYSTEM "Xdmf.dtd" []>
    <Xdmf Version="3.0">
    <Domain>
        <Grid Name="TimeSeries" GridType="Collection" CollectionType="Temporal">'''

    # XDMF END
    xdmf_str_end = '''</Grid>
    </Domain>
    </Xdmf>'''

    # JOIN FINAL XDMF STR AND WRITE TO FILE
    xdmf_str = "\n".join([xdmf_str_start] + xdmf_timeseries + [xdmf_str_end])
    with open(xdmffile_path, "w") as xdmf_file:
        xdmf_file.write(xdmf_str)

def generate_paraview_pngs(
        pvbatch_executable_path: str, 
        data_path: str,
        save_path: str,
        start: int = 0,
        stop: int = None,
        step: int = 1,
        naming_offset: int = 0,
        contour_keys: Union[List, Tuple] = None,
        contour_values: Union[List, Tuple] = None, 
        contour_color_keys: Union[List, Tuple] = None,
        contour_cmaps: Union[List, Tuple] = None,
        contour_opacities: Union[List, Tuple] = None,
        contour_color_lims: Union[List, Tuple] = None,
        contour_speculars: Union[List, Tuple] = None,
        contour_opacity_mappings: Union[List, Tuple] = None,
        slice_keys: Union[List, Tuple] = None,
        slice_origins: Union[List, Tuple] = None,
        slice_normals: Union[List, Tuple] = None,
        slice_logs: Union[List, Tuple] = None,
        slice_cmaps: Union[List, Tuple] = None,
        slice_opacities: Union[List, Tuple] = None,
        slice_color_lims: Union[List, Tuple] = None,
        slice_translations: Union[List, Tuple] = None,
        camera_position: Tuple = None,
        camera_focal_point: Tuple = None,
        camera_view_up: Tuple = None, 
        camera_view_angle: Tuple = None,
        camera_dolly: Union[List, Tuple] =None,
        is_orientation_axis: bool = False,
        background_color: Union[str, List, Tuple] = None,
        resolution: Union[List, Tuple] = None
        ) -> None:
    """_summary_
    TODO deniz
    :param pvbatch_executable_path: _description_
    :type pvbatch_executable_path: str
    :param data_path: _description_
    :type data_path: str
    :param save_path: _description_
    :type save_path: str
    :param start: _description_, defaults to 0
    :type start: int, optional
    :param stop: _description_, defaults to None
    :type stop: int, optional
    :param step: _description_, defaults to 1
    :type step: int, optional
    :param naming_offset: _description_, defaults to 0
    :type naming_offset: int, optional
    :param contour_keys: _description_, defaults to None
    :type contour_keys: Union[List, Tuple], optional
    :param contour_values: _description_, defaults to None
    :type contour_values: Union[List, Tuple], optional
    :param contour_color_keys: _description_, defaults to None
    :type contour_color_keys: Union[List, Tuple], optional
    :param contour_cmaps: _description_, defaults to None
    :type contour_cmaps: Union[List, Tuple], optional
    :param contour_opacities: _description_, defaults to None
    :type contour_opacities: Union[List, Tuple], optional
    :param contour_color_lims: _description_, defaults to None
    :type contour_color_lims: Union[List, Tuple], optional
    :param contour_speculars: _description_, defaults to None
    :type contour_speculars: Union[List, Tuple], optional
    :param contour_opacity_mappings: _description_, defaults to None
    :type contour_opacity_mappings: Union[List, Tuple], optional
    :param slice_keys: _description_, defaults to None
    :type slice_keys: Union[List, Tuple], optional
    :param slice_origins: _description_, defaults to None
    :type slice_origins: Union[List, Tuple], optional
    :param slice_normals: _description_, defaults to None
    :type slice_normals: Union[List, Tuple], optional
    :param slice_logs: _description_, defaults to None
    :type slice_logs: Union[List, Tuple], optional
    :param slice_cmaps: _description_, defaults to None
    :type slice_cmaps: Union[List, Tuple], optional
    :param slice_opacities: _description_, defaults to None
    :type slice_opacities: Union[List, Tuple], optional
    :param slice_color_lims: _description_, defaults to None
    :type slice_color_lims: Union[List, Tuple], optional
    :param slice_translations: _description_, defaults to None
    :type slice_translations: Union[List, Tuple], optional
    :param camera_position: _description_, defaults to None
    :type camera_position: Tuple, optional
    :param camera_focal_point: _description_, defaults to None
    :type camera_focal_point: Tuple, optional
    :param camera_view_up: _description_, defaults to None
    :type camera_view_up: Tuple, optional
    :param camera_view_angle: _description_, defaults to None
    :type camera_view_angle: Tuple, optional
    :param camera_dolly: _description_, defaults to None
    :type camera_dolly: Union[List, Tuple], optional
    :param is_orientation_axis: _description_, defaults to False
    :type is_orientation_axis: bool, optional
    :param background_color: _description_, defaults to None
    :type background_color: Union[str, List, Tuple], optional
    :param resolution: _description_, defaults to None
    :type resolution: Union[List, Tuple], optional
    :raises NotImplementedError: _description_
    """

    # ASSERTS AND CHECKS
    assert_string = "Consistency error. pvbatch_executable_path does not exist."
    assert os.path.exists(pvbatch_executable_path), assert_string
    
    assert_string = "Consistency error. camera_focal point has to be of type tuple or None."
    assert isinstance(camera_focal_point, (tuple, None)), assert_string

    if contour_keys is not None:
        assert_string = "contour_values must not be None if contour_keys are provided."
        assert contour_values is not None, assert_string
        assert_string = "contour_color_keys must not be None if contour_keys are provided."
        assert contour_color_keys is not None, assert_string

        no_contours = len(contour_keys)
        assert_string = "Length of contour_values must be equal to number of contours."
        assert no_contours == len(contour_values), assert_string
        assert_string = "Length of contour_color_keys must be equal to number of contours."
        assert no_contours == len(contour_color_keys), assert_string
    
    if slice_keys is not None:
        assert_string = "slice_origins must not be None if slice_keys are provided."
        assert slice_origins is not None, assert_string
        assert_string = "slice_normals must not be None if slice_keys are provided."
        assert slice_normals is not None, assert_string

        no_slices = len(slice_keys)
        assert_string = "Length of slice_origins must be equal to number of slices."
        assert no_slices == len(slice_origins), assert_string
        assert_string = "Length of slice_normals must be equal to number of slices."
        assert no_slices == len(slice_normals), assert_string

    # SEARCH DIRECTORY FOR FILES
    files = []
    times = []
    for file in os.listdir(data_path):
        if file.endswith("xdmf"):
            file_basename = os.path.splitext(os.path.basename(file))[0]
            if file_basename == "data_time_series":
                continue
            files.append(file)
            times.append(float(file_basename[5:]))
    indices = np.argsort(np.array(times))
    files   = np.array(files)[indices][start:stop:step]
    files = [os.path.join(data_path, file) for file in files]

    # BASE COMMAND
    file_path = os.path.abspath(__file__)
    pv_script_path = os.path.join(os.path.split(file_path)[0], "paraview_control_3D.py")

    pv_shell_command = "%s %s --files %s --save_path %s --naming_offset %i" %(
        pvbatch_executable_path, pv_script_path, " ".join(files), save_path, naming_offset)

    # CONTOURS 
    if contour_keys and contour_values and contour_color_keys:
        contour_command = ""
        for ii, (contour_ii_key, contour_ii_value, contour_ii_color_key)  in enumerate(zip(contour_keys, contour_values, contour_color_keys)):
            contour_command += " --contour_keys %s --contour_values %s --contour_color_keys %s" % (contour_ii_key, contour_ii_value, contour_ii_color_key)

            if contour_cmaps:
                contour_command += " --contour_cmaps %s" % contour_cmaps[ii] 
            if contour_opacities:
                contour_command += " --contour_opacities %s" % contour_opacities[ii]
            if contour_color_lims:
                if contour_color_lims[ii]:
                    contour_command += " --contour_color_lims %f %f" % contour_color_lims[ii] 
                else:
                    contour_command += " --contour_color_lims None" 
            if contour_speculars:
                contour_command += " --contour_speculars %s" % contour_speculars[ii]
            if contour_opacity_mappings:
                contour_command += " --contour_opacity_mappings %s" % contour_opacity_mappings[ii]

        pv_shell_command += contour_command

    # SLICES
    if slice_keys and slice_origins and slice_normals:
        slices_command = ""
        for ii, (slice_ii_key, slice_ii_origin, slice_ii_normal)  in enumerate(zip(slice_keys, slice_origins, slice_normals)):
            slices_command += " --slice_keys %s --slice_origins %s %s %s --slice_normals %s %s %s" % (slice_ii_key, *slice_ii_origin, *slice_ii_normal)

            if slice_logs:
                slices_command += " --slice_logs %s" % slice_logs[ii] or "False"
            if slice_cmaps:
                slices_command += " --slice_cmaps %s" % slice_cmaps[ii]
            if slice_opacities:
                slices_command += " --slice_opacities %s" % slice_opacities[ii]
            if slice_color_lims:
                if slice_color_lims[ii]:
                    slices_command += " --slice_color_lims %f %f" % slice_color_lims[ii] 
                else:
                    slices_command += " --slice_color_lims None"
            if slice_translations:
                slices_command += " --slice_translations %s %s %s" % (*slice_translations[ii],)
        
        pv_shell_command += slices_command

    # CAMERA
    if camera_position:
        if isinstance(camera_position, str):
            pv_shell_command += " --camera_position %s" % camera_position
        elif isinstance(camera_position, tuple):
            pv_shell_command += " --camera_position %f %f %f" % camera_position
        else:
            assert_string = ("camera_position has to be of type str or tuple "
                             f"but is of type {type(camera_position)}.")
            assert True, assert_string
    if camera_focal_point:
        if isinstance(camera_focal_point, str):
            pv_shell_command += " --camera_focal_point %s" % camera_focal_point
        elif isinstance(camera_position, (tuple, list)):
            pv_shell_command += " --camera_focal_point %f %f %f" % camera_focal_point
        else:
            assert_string = ("camera_focal_point has to be of type str or tuple "
                             f"but is of type {type(camera_focal_point)}.")
            assert True, assert_string
    if camera_view_angle:
        pv_shell_command += " --camera_view_angle %f %f %f" % camera_view_angle
    if camera_view_up:
        pv_shell_command += " --camera_view_up %f %f %f" % camera_view_up
    if camera_dolly:
        pv_shell_command += " --camera_dolly %f" % camera_dolly

    # MISCELLANEOUS
    if is_orientation_axis:
        pv_shell_command += " --is_orientation_axis"
    if background_color:
        if isinstance(background_color, str):
            pv_shell_command += " --background_color %s" %background_color
        elif isinstance(background_color, (tuple, list)):
            pv_shell_command += " --background_color %f %f %f" %background_color
        else:
            raise NotImplementedError
    if resolution:
        pv_shell_command += " --resolution %i %i" % resolution

    # RUN COMMAND
    subprocess.run([pv_shell_command], shell=True)

def generate_paraview_pngs_2D(
        pvbatch_executable_path: str, 
        data_path: str,
        save_path: str,
        start: int = 0,
        stop: int = None,
        step: int = 1,
        naming_offset: int = 0,
        field_key: str = None,
        field_cmap: str = None,
        field_color_lims: Union[List, Tuple] = None,
        field_log_scaling: bool = False,
        contour_keys: Union[List, Tuple] = None,
        contour_values: Union[List, Tuple] = None, 
        contour_color_keys: Union[List, Tuple] = None,
        contour_cmaps: Union[List, Tuple] = None,
        contour_opacities: Union[List, Tuple] = None,
        contour_color_lims: Union[List, Tuple] = None,
        contour_speculars: Union[List, Tuple] = None,
        contour_opacity_mappings: Union[List, Tuple] = None,
        camera_position: Tuple = None,
        camera_focal_point: Tuple = None,
        camera_view_up: Tuple = None, 
        camera_view_angle: Tuple = None,
        camera_dolly: Union[List, Tuple] =None,
        is_orientation_axis: bool = False,
        background_color: Union[str, List, Tuple] = None,
        resolution: Union[List, Tuple] = None
        ) -> None:

    # ASSERTS AND CHECKS
    assert_string = "Consistency error. pvbatch_executable_path does not exist."
    assert os.path.exists(pvbatch_executable_path), assert_string
    
    assert_string = "Consistency error. camera_focal point has to be of type tuple or None."
    assert isinstance(camera_focal_point, (tuple, None)), assert_string

    if contour_keys is not None:
        assert_string = "contour_values must not be None if contour_keys are provided."
        assert contour_values is not None, assert_string
        assert_string = "contour_color_keys must not be None if contour_keys are provided."
        assert contour_color_keys is not None, assert_string

        no_contours = len(contour_keys)
        assert_string = "Length of contour_values must be equal to number of contours."
        assert no_contours == len(contour_values), assert_string
        assert_string = "Length of contour_color_keys must be equal to number of contours."
        assert no_contours == len(contour_color_keys), assert_string
    
    # SEARCH DIRECTORY FOR FILES
    files = []
    times = []
    for file in os.listdir(data_path):
        if file.endswith("xdmf"):
            file_basename = os.path.splitext(os.path.basename(file))[0]
            if file_basename == "data_time_series":
                continue
            files.append(file)
            times.append(float(file_basename[5:]))
    indices = np.argsort(np.array(times))
    files   = np.array(files)[indices][start:stop:step]
    files = [os.path.join(data_path, file) for file in files]

    # BASE COMMAND
    file_path = os.path.abspath(__file__)
    pv_script_path = os.path.join(os.path.split(file_path)[0], "paraview_control_2D.py")

    pv_shell_command = "%s %s --files %s --save_path %s --naming_offset %i" %(
        pvbatch_executable_path, pv_script_path, " ".join(files), save_path, naming_offset)

    # FIELD
    if field_key:
        field_command = f" --field_key {field_key}"
        if field_cmap:
            field_command += f" --field_cmap {field_cmap}"
        if field_color_lims:
            field_command += f" --field_color_lims {field_color_lims[0]} {field_color_lims[1]}"
        if field_log_scaling:
            field_command += f" --field_log_scaling {field_log_scaling}"
        pv_shell_command += field_command

    # CONTOURS 
    if contour_keys and contour_values and contour_color_keys:
        contour_command = ""
        for ii, (contour_ii_key, contour_ii_value, contour_ii_color_key)  in enumerate(zip(contour_keys, contour_values, contour_color_keys)):
            contour_command += " --contour_keys %s --contour_values %s --contour_color_keys %s" % (contour_ii_key, contour_ii_value, contour_ii_color_key)

            if contour_cmaps:
                contour_command += " --contour_cmaps %s" % contour_cmaps[ii] 
            if contour_opacities:
                contour_command += " --contour_opacities %s" % contour_opacities[ii]
            if contour_color_lims:
                if contour_color_lims[ii]:
                    contour_command += " --contour_color_lims %f %f" % contour_color_lims[ii] 
                else:
                    contour_command += " --contour_color_lims None" 
            if contour_speculars:
                contour_command += " --contour_speculars %s" % contour_speculars[ii]
            if contour_opacity_mappings:
                contour_command += " --contour_opacity_mappings %s" % contour_opacity_mappings[ii]

        pv_shell_command += contour_command

    # CAMERA
    if camera_position:
        if isinstance(camera_position, str):
            pv_shell_command += " --camera_position %s" % camera_position
        elif isinstance(camera_position, tuple):
            pv_shell_command += " --camera_position %f %f %f" % camera_position
        else:
            assert_string = ("camera_position has to be of type str or tuple "
                             f"but is of type {type(camera_position)}.")
            assert True, assert_string
    if camera_focal_point:
        if isinstance(camera_focal_point, str):
            pv_shell_command += " --camera_focal_point %s" % camera_focal_point
        elif isinstance(camera_position, (tuple, list)):
            pv_shell_command += " --camera_focal_point %f %f %f" % camera_focal_point
        else:
            assert_string = ("camera_focal_point has to be of type str or tuple "
                             f"but is of type {type(camera_focal_point)}.")
            assert True, assert_string
    if camera_view_angle:
        pv_shell_command += " --camera_view_angle %f %f %f" % camera_view_angle
    if camera_view_up:
        pv_shell_command += " --camera_view_up %f %f %f" % camera_view_up
    if camera_dolly:
        pv_shell_command += " --camera_dolly %f" % camera_dolly

    # MISCELLANEOUS
    if is_orientation_axis:
        pv_shell_command += " --is_orientation_axis"
    if background_color:
        if isinstance(background_color, str):
            pv_shell_command += " --background_color %s" %background_color
        elif isinstance(background_color, (tuple, list)):
            pv_shell_command += " --background_color %f %f %f" %background_color
        else:
            raise NotImplementedError
    if resolution:
        pv_shell_command += " --resolution %i %i" % resolution

    # RUN COMMAND
    subprocess.run([pv_shell_command], shell=True)

####################################################################################################
# Deprecated functionalities

def load_data_old(
        path: str,
        quantities: List,
        start: int = 0,
        stop: int = None,
        step: int = 1,
        verbose: bool = True,
        reassemble = True,
        reassemble_jnp = False,
        ) -> Tuple[Tuple[np.ndarray, np.ndarray, np.ndarray],
                   Tuple[np.ndarray, np.ndarray, np.ndarray],
                   np.ndarray, Dict[str, np.ndarray]]:
    """Loads .h5 result files generated by a jaxfluids simulation.

    :param path: Path to the folder containing the .h5 files for the time snapshots
    :type path: str
    :param quantities: Quantities to load
    :type quantities: List
    :param start: Starting time snapshot to load, defaults to 0
    :type start: int, optional
    :param stop: Ending time snapshot to load, defaults to None
    :type stop: int, optional
    :param step: Interval of time snapshots to load, defaults to 1
    :type step: int, optional
    :return: Cell centers coordinates, cell sizes, times and dictionary of buffers
    :rtype: Tuple
    """
    warn("""The function load_data_old() is deprecated and will not be supported
        in upcoming JAX-Fluids versions.\n\n""", DeprecationWarning,
        stacklevel=2)

    assert os.path.exists(path), f"Given path {path:s} does not exist."
    if isinstance(stop, int):
        assert stop > start, "Stop argument in load_data must be larger than start."
    is_h5files = len(glob.glob(os.path.join(path, "*.h5"))) > 0
    if path.endswith("domain"):
        assert is_h5files, f"No h5 files found in {path:s}."
    else:
        if not is_h5files:
            path_ = os.path.join(path, "domain")
            is_h5files = len(glob.glob(os.path.join(path_, "*.h5"))) > 0
            assert is_h5files, f"No h5 files in {path:s} or in {path_:s}"
            path = path_

    # SEARCH DIRECTORY FOR FILES
    files = []
    times = []
    for file in os.listdir(path):
        if file.endswith("h5"):
            if "nan" in file:
                continue 
            files.append(file)
            times.append(float(os.path.splitext(os.path.basename(file))[0][5:]))
    indices     = np.argsort(np.array(times))
    times       = np.array(times)[indices][start:stop:step]
    files       = np.array(files)[indices][start:stop:step]
    no_times    = len(times)

    # METADATA AND GRID
    with h5py.File(os.path.join(path, files[0]), "r") as h5file:
        x = h5file["domain/gridX"][:]
        y = h5file["domain/gridY"][:]
        z = h5file["domain/gridZ"][:]
        dx = h5file["domain/cellsizeX"][()]
        dy = h5file["domain/cellsizeY"][()]
        dz = h5file["domain/cellsizeZ"][()]
        split_factors = tuple(h5file["domain"]["split_factors"][:])

        is_parallel = h5file["metadata"]["is_parallel"][()]
        diffuse_interface_model = h5file["metadata"]["diffuse_interface_model"][()]
        if type(diffuse_interface_model) != np.bool_:
            diffuse_interface_model = diffuse_interface_model.decode("utf-8")
        levelset_model = h5file["metadata"]["levelset_model"][()]
        if type(levelset_model) != np.bool_:
            levelset_model = levelset_model.decode("utf-8")
        number_fluids = h5file["metadata"]["number_fluids"][()]
        is_double = h5file["metadata"]["is_double_precision"][()]
        if is_double:
            precision.enable_double_precision()
        dtype = np.float64 if is_double else np.float32

        cell_centers = (x,y,z)
        cell_sizes = (dx,dy,dz)
        if is_parallel:
            cell_centers = reassemble_cell_centers_old(cell_centers, split_factors)
            cell_sizes = reassemble_cell_sizes_old(cell_sizes, split_factors)

        dim = sum([1 if len(xi) > 1 else 0 for xi in cell_centers])
        nx = len(cell_centers[0])
        ny = len(cell_centers[1])
        nz = len(cell_centers[2])

        number_of_cells = (nx,ny,nz)
        number_of_cells_device = tuple([int(number_of_cells[i]/split_factors[i]) for i in range(3)])

        available_quantities = {}
        for key in h5file["metadata/available_quantities"].keys():
            quantity_array = h5file["metadata/available_quantities"][key][:]
            available_quantities[key] = [arr.decode("utf-8") for arr in quantity_array]

    # CHECK IF REQUESTED QUANTITIES ARE AVAILABLE
    quantity_to_field_type = {}
    for quantity in quantities:
        check = False
        # IDENTIFY FIELD TYPE
        if quantity.split("_")[0] == "real":
            if quantity.split("_")[1] in available_quantities["real_fluid"]:
                quantity_to_field_type[quantity] = "real_fluid"
                check = True
        else:
            for field_type in available_quantities:
                if field_type == "real_fluid":
                    continue
                if quantity in available_quantities[field_type]:
                    quantity_to_field_type[quantity] = field_type
                    check = True

        assert check, f"Quantity {quantity} not available in .h5 file"

    if is_parallel and not reassemble:
        # raise NotImplementedError #TODO AARON PARALLEL PARAVIEW TRANSPOSE
        nx,ny,nz = number_of_cells_device
        no_subdomains = np.prod(np.array(split_factors))
        shape = (no_subdomains,nx,ny,nz)
    else:
        nx,ny,nz = number_of_cells
        shape = (nx,ny,nz)

    # CREATE BUFFERS
    data_dictionary = {}
    for quantity in quantities:
        field_type = quantity_to_field_type[quantity]

        # PRIMITIVES AND CONSERVATIVES
        if field_type in ["primitives", "conservatives"]:
            if levelset_model == "FLUID-FLUID":
                if quantity in ["velocity", "momentum"]:
                    data_dictionary[quantity] = np.zeros((no_times, 2, dim) + shape, dtype)
                else:
                    data_dictionary[quantity] = np.zeros((no_times, 2) + shape, dtype)

            else:
                if quantity in ["velocity", "momentum"]:
                    data_dictionary[quantity] = np.zeros((no_times, dim) + shape, dtype)
                else:
                    data_dictionary[quantity] = np.zeros((no_times,) + shape, dtype)

        # REAL FLUID
        elif field_type in ["levelset", "real_fluid"]:
            if quantity in ["normal", "real_velocity", "real_momentum"]:
                data_dictionary[quantity] = np.zeros((no_times, dim) + shape, dtype)
            elif quantity == "interface_pressure":
                data_dictionary[quantity] = np.zeros((no_times, 2) + shape, dtype)
            else:
                data_dictionary[quantity] = np.zeros((no_times,) + shape, dtype)

        # MISCELLANEOUS
        elif field_type == "miscellaneous":
            if quantity == "vorticity":
                data_dictionary[quantity] = np.zeros((no_times, 1 if dim==2 else 3) + shape, dtype)
            else:
                data_dictionary[quantity] = np.zeros((no_times,) + shape, dtype)

        # MASS FLOW FORCING
        elif field_type == "forcings":
            data_dictionary[quantity] = np.zeros(no_times)


    # FILL BUFFERS
    for i, file in enumerate(files):
        if verbose:
            print("Loading time snapshot %.4e" % times[i])
        with h5py.File(os.path.join(path, file), "r") as h5file:
            
            for quantity in quantities:

                field_type = quantity_to_field_type[quantity]
                
                # PRIMITIVES
                if field_type == "primitives":
                    for j in range(2 if levelset_model == "FLUID-FLUID" else 1):
                        quantity_name_h5 = quantity + "_%d" % j if levelset_model == "FLUID-FLUID" else quantity
                        s_1 = np.s_[i,j] if levelset_model == "FLUID-FLUID" else np.s_[i]
                        if is_parallel:
                            buffer = h5file["primitives/" + quantity_name_h5][:]
                            if reassemble:
                                buffer = reassemble_buffer_old(buffer, split_factors, reassemble_jnp)
                            else:
                                buffer = jnp.swapaxes(buffer, -3, -1)
                        else:
                            buffer = h5file["primitives/" + quantity_name_h5][:].T
                        data_dictionary[quantity][s_1] = buffer

                # CONSERVATIVES
                elif field_type == "conservatives":
                    for j in range(2 if levelset_model == "FLUID-FLUID" else 1):
                        quantity_name_h5 = quantity + "_%d" % j if levelset_model == "FLUID-FLUID" else quantity
                        s_1 = np.s_[i,j] if levelset_model == "FLUID-FLUID" else np.s_[i]
                        if is_parallel:
                            buffer = h5file["conservatives/" + quantity_name_h5][:]
                            if reassemble:
                                buffer = reassemble_buffer_old(buffer, split_factors, reassemble_jnp)
                            else:
                                buffer = jnp.swapaxes(buffer, -3, -1)
                        else:
                            buffer = h5file["conservatives/" + quantity_name_h5][:].T
                        data_dictionary[quantity][s_1] = buffer

                # LEVELSET
                elif field_type == "levelset":
                    if is_parallel:
                        buffer = h5file["levelset/" + quantity][:]
                        if reassemble:
                            buffer = reassemble_buffer_old(buffer, split_factors, reassemble_jnp)
                        else:
                            buffer = jnp.swapaxes(buffer, -3, -1)
                    else:
                        buffer = h5file["levelset/" + quantity][:].T
                    data_dictionary[quantity][i] = buffer

                # REAL FLUID
                elif field_type == "real_fluid":
                    quantity_name_h5 = quantity.split("_")[1]
                    if is_parallel:
                        buffer = h5file["real_fluid/" + quantity_name_h5][:]
                        if reassemble:
                            buffer = reassemble_buffer_old(buffer, split_factors, reassemble_jnp)
                        else:
                            buffer = jnp.swapaxes(buffer, -3, -1)
                    else:
                        buffer = h5file["real_fluid/" + quantity_name_h5][:].T
                    data_dictionary[quantity][i] = buffer

                # MISCELLANEOUS
                elif field_type == "miscellaneous":
                    if is_parallel:
                        buffer = h5file["miscellaneous/" + quantity][:]
                        if reassemble:
                            buffer = reassemble_buffer_old(buffer, split_factors, reassemble_jnp)
                        else:
                            buffer = jnp.swapaxes(buffer, -3, -1)
                    else:
                        buffer = h5file["miscellaneous/" + quantity][:].T
                    data_dictionary[quantity][i] = buffer                

                # MASS FLOW FORCING
                elif field_type == "forcings":
                    data_dictionary[quantity][i] = h5file["forcings/mass_flow/force_scalar"][()]

    return cell_centers, cell_sizes, times, data_dictionary

def reassemble_buffer_old(
        buffer: np.ndarray,
        split_factors: Tuple,
        jax_numpy = False,
        keep_transpose = False,
        )-> np.ndarray:
    """Reassembles a decomposed buffer.
    jax_numpy specifies whether jax or
    numpy operators are used.

    :param buffer: Buffer with shape (Ni,Nz+2*Nh,Ny+2*Nh,Nx+2*Nh,...)
        where Ni is the number of subdomains, Nx, Ny, Nz, are
        the number of cells, and Nh are the number of halo
        cells
    :type buffer: Array
    :param split_factors: Specifies the domain decomposition, defaults to None
    :type split_factors: Tuple, optional
    :param nh: Number of halo cells, defaults to None
    :type nh: int, optional
    :return: _description_
    :rtype: Array
    """
    if jax_numpy:
        return reassemble_buffer_jnp_old(buffer, split_factors, keep_transpose)
    else:
        return reassemble_buffer_np_old(buffer, split_factors, keep_transpose)

from functools import partial
@partial(jax.jit, static_argnums=(1,2))
def reassemble_buffer_jnp_old(
        buffer: Array,
        split_factors: Tuple,
        keep_transpose: bool
        ) -> Array:
    shape = buffer.shape
    reshape = tuple(split_factors) + shape[1:]
    buffer = jnp.reshape(buffer, reshape)
    buffer = jnp.concatenate([buffer[i] for i in range(split_factors[0])], axis=4)
    buffer = jnp.concatenate([buffer[i] for i in range(split_factors[1])], axis=2)
    buffer = jnp.concatenate([buffer[i] for i in range(split_factors[2])], axis=0)
    if not keep_transpose:
        buffer = jnp.transpose(buffer)
    return buffer

def reassemble_buffer_np_old(
        buffer: np.ndarray,
        split_factors: Tuple,
        keep_transpose: bool
        ) -> np.ndarray:
    shape = buffer.shape
    reshape = tuple(split_factors) + shape[1:]
    buffer = np.reshape(buffer, reshape)
    buffer = np.concatenate([buffer[i] for i in range(split_factors[0])], axis=4)
    buffer = np.concatenate([buffer[i] for i in range(split_factors[1])], axis=2)
    buffer = np.concatenate([buffer[i] for i in range(split_factors[2])], axis=0)
    if not keep_transpose:
        buffer = np.transpose(buffer)
    return buffer

def reassemble_cell_centers_old(
        cell_centers: Tuple[np.ndarray],
        split_factors: Tuple
        ) -> np.ndarray:
    cell_centers_xi = []
    for i in range(3):
        xi = cell_centers[i]
        xi = split_subdomain_dimensions_old(xi, split_factors)
        if split_factors[i] == 1:
            xi = xi[0,0,0]
        else:
            xi = np.concatenate([
                xi[tuple(np.roll([k,0,0], i))] for k in range(split_factors[i])
            ], axis=-1)
        cell_centers_xi.append(xi)

    return cell_centers_xi

def reassemble_cell_sizes_old(
        cell_sizes: Tuple[np.ndarray],
        split_factors: Tuple
        ) -> np.ndarray:
    cell_sizes_xi = []
    for i in range(3):
        dxi = cell_sizes[i]
        if dxi.ndim == 4:
            dxi = split_subdomain_dimensions_old(dxi, split_factors)
            dxi = np.concatenate([
                dxi[tuple(np.roll([k,0,0], i))] for k in range(split_factors[i])
            ], axis=-3+i)
            # dxi = np.concatenate([dxi[k] for k in range(split_factors[0])], axis=-3)
            # dxi = np.concatenate([dxi[k] for k in range(split_factors[1])], axis=-2)
            # dxi = np.concatenate([dxi[k] for k in range(split_factors[2])], axis=-1)
        cell_sizes_xi.append(dxi)

    return cell_sizes_xi


def split_subdomain_dimensions_old(
    buffer: np.ndarray,
    split_factors: Tuple
    ) -> np.ndarray:
    """Splits up the subdomain dimensions of the buffer.

    :param buffer: _description_
    :type buffer: np.ndarray
    :return: _description_
    :rtype: np.ndarray
    """
    shape = buffer.shape
    reshape = tuple(split_factors) + shape[1:]
    buffer = buffer.reshape(reshape)
    return buffer

def flatten_subdomain_dimensions_old(
    buffer: np.ndarray
    ) -> np.ndarray:
    """Flattens the subdomain dimensions of the buffer.

    :param buffer: _description_
    :type buffer: np.ndarray
    :return: _description_
    :rtype: List
    """
    shape = buffer.shape
    reshape = (-1,) + shape[3:]
    buffer = buffer.reshape(reshape)
    return buffer
