{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cd0d151",
   "metadata": {},
   "source": [
    "# Neural Network Training Using JAX-Fluids\n",
    "\n",
    "This notebook sketches one possibility of training neural networks that are embedded into the JAX-Fluids solver. We consider the following toy problem:\n",
    "\n",
    "Given the target velocity profile $u(y)$, i.e., the velocity in x-direction as a function of y, we are interested in learning the forcing term $S_x(y)$. We neglect the convective fluxes and only solve for the dissipative fluxes $F^d$.\n",
    "\n",
    "$\\frac{\\partial U}{\\partial t} = \\nabla \\cdot F^d + S_x$\n",
    "\n",
    "The forcing is modeled by a neural network $NN_{\\theta}$ which receives the y-coordinates as input.\n",
    "\n",
    "$S_x = NN_{\\theta}(y)$\n",
    "\n",
    "$\\theta$ are weights and biases of the neural network.\n",
    "\n",
    "The true forcing is $F_x(y) = \\sin (2 * \\pi * y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c3843",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import jax\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from flax import linen as nn\n",
    "import optax\n",
    "\n",
    "from jaxfluids import InputManager, InitializationManager, SimulationManager\n",
    "from jaxfluids.feed_forward.data_types import FeedForwardSetup\n",
    "from jaxfluids.data_types.ml_buffers import ParametersSetup\n",
    "from jaxfluids.callbacks import Callback\n",
    "from jaxfluids_postprocess import load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd96b33",
   "metadata": {},
   "source": [
    "The neural network is a simple MLP with two hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2045768",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x[...,None]\n",
    "        x = nn.Dense(32)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.Dense(32)(x)\n",
    "        x = nn.tanh(x)\n",
    "        x = nn.Dense(1)(x)\n",
    "        x = x[...,0]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21634a3",
   "metadata": {},
   "source": [
    "The forcing is implemented via a Callback. Here, the MLP is evaluated and the resulting forcing of the x-velocity is added to the right-hand side buffer (rhs_buffers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786e7351",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForceCallback(Callback):\n",
    "\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def after_compute_rhs(self, rhs_buffers, ml_setup, **kwargs):\n",
    "\n",
    "        params = ml_setup.parameters.callbacks\n",
    "        \n",
    "        X, Y = self.domain_information.compute_device_mesh_grid()\n",
    "        force_u = self.model.apply(params, Y)\n",
    "\n",
    "        rhs_conservatives = rhs_buffers.euler_buffers.conservatives\n",
    "        rhs_conservatives = rhs_conservatives.at[1].add(force_u)\n",
    "\n",
    "        rhs_buffers = rhs_buffers._replace(\n",
    "            euler_buffers=rhs_buffers.euler_buffers._replace(\n",
    "                conservatives=rhs_conservatives\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return rhs_buffers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dfd943",
   "metadata": {},
   "source": [
    "Training data is generated by running the forward simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6375e832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data():\n",
    "    input_manager = InputManager(\n",
    "        \"shear_flow.json\",\n",
    "        \"numerical_setup.json\"\n",
    "    )\n",
    "    init_manager = InitializationManager(input_manager)\n",
    "    sim_manager = SimulationManager(input_manager)\n",
    "\n",
    "    jxf_buffers = init_manager.initialization()\n",
    "    sim_manager.simulate(jxf_buffers)\n",
    "\n",
    "    return sim_manager.output_writer.save_path_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a6620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_data(data_path):\n",
    "    jxf_data = load_data(data_path, quantities=[\"density\", \"velocity\", \"pressure\"])\n",
    "\n",
    "    x,y,z = jxf_data.cell_centers\n",
    "    density = jxf_data.data[\"density\"]\n",
    "    velocity = jxf_data.data[\"velocity\"]\n",
    "    pressure = jxf_data.data[\"pressure\"]\n",
    "\n",
    "    primitives_init = jnp.stack([\n",
    "        density[0],\n",
    "        velocity[0,0],\n",
    "        velocity[0,1],\n",
    "        jnp.zeros_like(density[0]),\n",
    "        pressure[0]\n",
    "    ], axis=0)\n",
    "\n",
    "    velX_target = jxf_data.data[\"velocity\"][-1,0]\n",
    "\n",
    "    return primitives_init, velX_target, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9bbeb",
   "metadata": {},
   "source": [
    "We define the step function which consists of the following steps:\n",
    "- Execute the forward pass by calling the feed_forward method of the SimulationManager\n",
    "- Calculates the loss: Here, the squared difference between the target velocity profile and the velocity profile obtained from the forward pass\n",
    "- Compute the gradient of the loss with respect to network parameters\n",
    "- Perform a gradient descent step\n",
    "\n",
    "Note: Gradients are propagated through the entire simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032cefc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_step_function(model, optimizer):\n",
    "\n",
    "    case_setup_dict = json.load(open(\"shear_flow.json\", \"r\"))\n",
    "    numerical_setup_dict = json.load(open(\"numerical_setup.json\", \"r\"))\n",
    "    numerical_setup_dict[\"active_forcings\"][\"is_custom_forcing\"] = False\n",
    "\n",
    "    force_cb = ForceCallback(model)\n",
    "\n",
    "    input_manager_train = InputManager(\n",
    "        case_setup_dict, numerical_setup_dict\n",
    "    )\n",
    "    sim_manager_train = SimulationManager(\n",
    "        input_manager_train,\n",
    "        callbacks=force_cb\n",
    "    )\n",
    "\n",
    "    feed_forward_setup = FeedForwardSetup(\n",
    "        outer_steps=200, inner_steps=1, is_scan=True\n",
    "    ) \n",
    "\n",
    "    def loss_fn(params, primitives_init, velX_target):\n",
    "        parameters_setup = ParametersSetup(callbacks=params)\n",
    "\n",
    "        solution_array, _ = sim_manager_train._feed_forward(\n",
    "            primes_init=primitives_init,\n",
    "            physical_timestep_size=1.5e-3,\n",
    "            t_start=0.0,\n",
    "            feed_forward_setup=feed_forward_setup,\n",
    "            ml_parameters=parameters_setup\n",
    "        )\n",
    "\n",
    "        primitives = solution_array[\"primitives\"][-1]\n",
    "        domain_information = sim_manager_train.domain_information\n",
    "        nhx, nhy, nhz = domain_information.domain_slices_conservatives\n",
    "        primitives = primitives[...,nhx,nhy,nhz]\n",
    "        velX = primitives[1]\n",
    "\n",
    "        loss = jnp.sum(jnp.square(velX - velX_target))\n",
    "        return loss, velX\n",
    "    \n",
    "    def step_function(params, opt_state, primitives_init, velX_target):\n",
    "        value_and_grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "        (loss, velX), grad = value_and_grad_fn(params, primitives_init, velX_target)\n",
    "        updates, opt_state = optimizer.update(grad, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "        return params, opt_state, loss, grad, velX\n",
    "\n",
    "    return step_function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32159268",
   "metadata": {},
   "source": [
    "In the training loop, we use the Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb98867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(data_path):\n",
    "\n",
    "    forcing = lambda y: jnp.sin(2 * jnp.pi * y)\n",
    "\n",
    "    primitives_init, velX_target, y = load_training_data(data_path)\n",
    "\n",
    "    model = MLP()\n",
    "    params = model.init(jax.random.key(42), jnp.ones((1,1,1)))\n",
    "    start_learning_rate = 1e-3\n",
    "    optimizer = optax.adam(start_learning_rate)\n",
    "    opt_state = optimizer.init(params)\n",
    "\n",
    "    step_fn = get_step_function(model, optimizer)\n",
    "    step_fn = jax.jit(step_fn)\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    for i in range(10000):\n",
    "        params, opt_state, loss, grad, velX = step_fn(\n",
    "            params, opt_state, primitives_init, velX_target\n",
    "        )\n",
    "        loss_history.append(loss)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            title_str = f\"STEP = {i}, LOSS = {loss:.2e}\"\n",
    "            print(title_str)\n",
    "\n",
    "            fig, ax = plt.subplots(ncols=3, figsize=(24,5))\n",
    "            fig.suptitle(title_str)\n",
    "            ax[0].plot(y, velX_target[0,:,0], label=\"target\")\n",
    "            ax[0].plot(y, primitives_init[1,0,:,0], color=\"gray\", linestyle=\"--\", label=r\"$t_0$\")\n",
    "            ax[0].plot(y, velX[0,:,0], linestyle=\"--\", label=r\"$t_N$\")\n",
    "            ax[0].legend()\n",
    "            ax[0].set_ylim([-0.2, 0.2])\n",
    "            ax[0].set_xlabel(r\"$y$\")\n",
    "            ax[0].set_ylabel(r\"$u$\")\n",
    "\n",
    "            ax[1].plot(y, forcing(y), label=\"True forcing\")\n",
    "            ax[1].plot(y, model.apply(params, y), linestyle=\"--\", label=\"NN forcing\")\n",
    "            ax[1].legend()\n",
    "            ax[1].set_ylim([-1.25, 1.25])\n",
    "            ax[1].set_xlabel(r\"$y$\")\n",
    "            ax[1].set_ylabel(r\"$F_x$\")\n",
    "\n",
    "            ax[2].plot(jnp.array(loss_history))\n",
    "            ax[2].set_xlabel(\"Steps\")\n",
    "            ax[2].set_ylabel(\"Loss\")\n",
    "            ax[2].set_yscale(\"log\")\n",
    "\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b422a1aa",
   "metadata": {},
   "source": [
    "As we can see, the neural network fairly quickly learns a forcing which results in a good approximation of the target velocity profile. Due to the dissipative nature of the underlying process, the final velocity profile is not very sensitive to small scale changes in the forcing term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74facfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = generate_training_data()\n",
    "train_model(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1bdfe7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_jaxfluids_github",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
